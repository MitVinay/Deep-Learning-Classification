{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1481789,"sourceType":"datasetVersion","datasetId":869651}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T02:35:45.647923Z","iopub.execute_input":"2023-11-27T02:35:45.648517Z","iopub.status.idle":"2023-11-27T02:35:45.966386Z","shell.execute_reply.started":"2023-11-27T02:35:45.648491Z","shell.execute_reply":"2023-11-27T02:35:45.965340Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/credit-card-customer-churn-prediction/Churn_Modelling.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/credit-card-customer-churn-prediction/Churn_Modelling.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:45.969126Z","iopub.execute_input":"2023-11-27T02:35:45.970104Z","iopub.status.idle":"2023-11-27T02:35:46.014117Z","shell.execute_reply.started":"2023-11-27T02:35:45.970056Z","shell.execute_reply":"2023-11-27T02:35:46.013004Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.015170Z","iopub.execute_input":"2023-11-27T02:35:46.016276Z","iopub.status.idle":"2023-11-27T02:35:46.050402Z","shell.execute_reply.started":"2023-11-27T02:35:46.016253Z","shell.execute_reply":"2023-11-27T02:35:46.049418Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(10000, 14)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0       2       0.00              1          1               1   \n1       1   83807.86              1          0               1   \n2       8  159660.80              3          1               0   \n3       1       0.00              2          0               0   \n4       2  125510.82              1          1               1   \n\n   EstimatedSalary  Exited  \n0        101348.88       1  \n1        112542.58       0  \n2        113931.57       1  \n3         93826.63       0  \n4         79084.10       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# checking missing values\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.052399Z","iopub.execute_input":"2023-11-27T02:35:46.052742Z","iopub.status.idle":"2023-11-27T02:35:46.076226Z","shell.execute_reply.started":"2023-11-27T02:35:46.052713Z","shell.execute_reply":"2023-11-27T02:35:46.075333Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        10000 non-null  int64  \n 1   CustomerId       10000 non-null  int64  \n 2   Surname          10000 non-null  object \n 3   CreditScore      10000 non-null  int64  \n 4   Geography        10000 non-null  object \n 5   Gender           10000 non-null  object \n 6   Age              10000 non-null  int64  \n 7   Tenure           10000 non-null  int64  \n 8   Balance          10000 non-null  float64\n 9   NumOfProducts    10000 non-null  int64  \n 10  HasCrCard        10000 non-null  int64  \n 11  IsActiveMember   10000 non-null  int64  \n 12  EstimatedSalary  10000 non-null  float64\n 13  Exited           10000 non-null  int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 1.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# checking total number of duplicated rows\ndf.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.077406Z","iopub.execute_input":"2023-11-27T02:35:46.077666Z","iopub.status.idle":"2023-11-27T02:35:46.092407Z","shell.execute_reply.started":"2023-11-27T02:35:46.077637Z","shell.execute_reply":"2023-11-27T02:35:46.091586Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Number of records of people enrolled to bank\ndf['Exited'].value_counts()\n# As we can see the below values, there is imbalance in the classification","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.093645Z","iopub.execute_input":"2023-11-27T02:35:46.093886Z","iopub.status.idle":"2023-11-27T02:35:46.100133Z","shell.execute_reply.started":"2023-11-27T02:35:46.093865Z","shell.execute_reply":"2023-11-27T02:35:46.099270Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Exited\n0    7963\n1    2037\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Geography'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.101505Z","iopub.execute_input":"2023-11-27T02:35:46.101736Z","iopub.status.idle":"2023-11-27T02:35:46.112348Z","shell.execute_reply.started":"2023-11-27T02:35:46.101716Z","shell.execute_reply":"2023-11-27T02:35:46.111466Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Geography\nFrance     5014\nGermany    2509\nSpain      2477\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Gender'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.113818Z","iopub.execute_input":"2023-11-27T02:35:46.114129Z","iopub.status.idle":"2023-11-27T02:35:46.124215Z","shell.execute_reply.started":"2023-11-27T02:35:46.114102Z","shell.execute_reply":"2023-11-27T02:35:46.123439Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Gender\nMale      5457\nFemale    4543\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Dropping irrelevant columns\ndf.drop(columns = [\"RowNumber\", \"CustomerId\", \"Surname\"], inplace = True)\n# inplace = True makes the changes permanant","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.125455Z","iopub.execute_input":"2023-11-27T02:35:46.125716Z","iopub.status.idle":"2023-11-27T02:35:46.136924Z","shell.execute_reply.started":"2023-11-27T02:35:46.125692Z","shell.execute_reply":"2023-11-27T02:35:46.136275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.139673Z","iopub.execute_input":"2023-11-27T02:35:46.140612Z","iopub.status.idle":"2023-11-27T02:35:46.154051Z","shell.execute_reply.started":"2023-11-27T02:35:46.140569Z","shell.execute_reply":"2023-11-27T02:35:46.153131Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n0          619    France  Female   42       2       0.00              1   \n1          608     Spain  Female   41       1   83807.86              1   \n2          502    France  Female   42       8  159660.80              3   \n3          699    France  Female   39       1       0.00              2   \n4          850     Spain  Female   43       2  125510.82              1   \n\n   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n0          1               1        101348.88       1  \n1          0               1        112542.58       0  \n2          1               0        113931.57       1  \n3          0               0         93826.63       0  \n4          1               1         79084.10       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# performing one-hot encoding for categorical data\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.get_dummies(df, columns=[\"Geography\", \"Gender\"], drop_first=True, dtype=np.int64)\n\n# We are dropping the firt column as 3 categories can be represented by 2 column and 2 category of gender column in one column","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.155802Z","iopub.execute_input":"2023-11-27T02:35:46.156085Z","iopub.status.idle":"2023-11-27T02:35:46.168150Z","shell.execute_reply.started":"2023-11-27T02:35:46.156061Z","shell.execute_reply":"2023-11-27T02:35:46.166999Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Scaling all the columns as we can see that the values of the column is not comparable\n# values of salary columns are much much higher\n# We are scaling beacuse in the time of calculation of weights it takes very long to converge\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.169423Z","iopub.execute_input":"2023-11-27T02:35:46.169879Z","iopub.status.idle":"2023-11-27T02:35:46.173565Z","shell.execute_reply.started":"2023-11-27T02:35:46.169854Z","shell.execute_reply":"2023-11-27T02:35:46.172674Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Splitting data\nX = df.drop(columns = ['Exited'])\ny = df['Exited']","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.174586Z","iopub.execute_input":"2023-11-27T02:35:46.175193Z","iopub.status.idle":"2023-11-27T02:35:46.188559Z","shell.execute_reply.started":"2023-11-27T02:35:46.175169Z","shell.execute_reply":"2023-11-27T02:35:46.187255Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:46.189664Z","iopub.execute_input":"2023-11-27T02:35:46.190095Z","iopub.status.idle":"2023-11-27T02:35:47.359766Z","shell.execute_reply.started":"2023-11-27T02:35:46.190070Z","shell.execute_reply":"2023-11-27T02:35:47.358469Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:47.361106Z","iopub.execute_input":"2023-11-27T02:35:47.361362Z","iopub.status.idle":"2023-11-27T02:35:47.367310Z","shell.execute_reply.started":"2023-11-27T02:35:47.361340Z","shell.execute_reply":"2023-11-27T02:35:47.366067Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(8000, 11)\n(2000, 11)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Scaling the training and test data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_trained_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n'''Scaling is a crucial step in many machine learning algorithms, and it involves standardizing or normalizing the features of your dataset. The reason for using different functions for scaling the training and testing data is to prevent data leakage and ensure that your model generalizes well to new, unseen data.\n\nWhen you scale your data, you calculate statistics (like mean and standard deviation) based on the training set and use these statistics to scale both the training and testing sets. This is why you use fit_transform on the training data—it calculates the scaling parameters (mean and standard deviation) and applies the transformation to the training set.\n\nHowever, when it comes to the testing set, you don't want to recalculate the scaling parameters because it's important that the scaling is consistent with what the model learned during training. If you used the statistics from the testing set to scale it, the model would essentially have information about the testing set during training, leading to optimistic and inaccurate performance estimates.\n\nSo, by using transform on the testing set, you apply the same scaling transformation that was learned from the training set. This ensures that the testing data is scaled in the same way as the training data, maintaining the integrity of the model evaluation on unseen data.'''\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:47.369081Z","iopub.execute_input":"2023-11-27T02:35:47.369449Z","iopub.status.idle":"2023-11-27T02:35:47.384264Z","shell.execute_reply.started":"2023-11-27T02:35:47.369426Z","shell.execute_reply":"2023-11-27T02:35:47.383342Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"\"Scaling is a crucial step in many machine learning algorithms, and it involves standardizing or normalizing the features of your dataset. The reason for using different functions for scaling the training and testing data is to prevent data leakage and ensure that your model generalizes well to new, unseen data.\\n\\nWhen you scale your data, you calculate statistics (like mean and standard deviation) based on the training set and use these statistics to scale both the training and testing sets. This is why you use fit_transform on the training data—it calculates the scaling parameters (mean and standard deviation) and applies the transformation to the training set.\\n\\nHowever, when it comes to the testing set, you don't want to recalculate the scaling parameters because it's important that the scaling is consistent with what the model learned during training. If you used the statistics from the testing set to scale it, the model would essentially have information about the testing set during training, leading to optimistic and inaccurate performance estimates.\\n\\nSo, by using transform on the testing set, you apply the same scaling transformation that was learned from the training set. This ensures that the testing data is scaled in the same way as the training data, maintaining the integrity of the model evaluation on unseen data.\""},"metadata":{}}]},{"cell_type":"code","source":"X_trained_scaled","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:47.385335Z","iopub.execute_input":"2023-11-27T02:35:47.385547Z","iopub.status.idle":"2023-11-27T02:35:47.393217Z","shell.execute_reply.started":"2023-11-27T02:35:47.385527Z","shell.execute_reply":"2023-11-27T02:35:47.392122Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([[-0.23082038, -0.94449979, -0.70174202, ...,  1.71490137,\n        -0.57273139,  0.91509065],\n       [-0.25150912, -0.94449979, -0.35520275, ..., -0.58312392,\n        -0.57273139, -1.09278791],\n       [-0.3963303 ,  0.77498705,  0.33787579, ...,  1.71490137,\n        -0.57273139, -1.09278791],\n       ...,\n       [ 0.22433188,  0.58393295,  1.3774936 , ..., -0.58312392,\n        -0.57273139, -1.09278791],\n       [ 0.13123255,  0.01077067,  1.03095433, ..., -0.58312392,\n        -0.57273139, -1.09278791],\n       [ 1.1656695 ,  0.29735181,  0.33787579, ...,  1.71490137,\n        -0.57273139,  0.91509065]])"},"metadata":{}}]},{"cell_type":"code","source":"pip install tensorFlow","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:35:47.394174Z","iopub.execute_input":"2023-11-27T02:35:47.394398Z","iopub.status.idle":"2023-11-27T02:35:58.011021Z","shell.execute_reply.started":"2023-11-27T02:35:47.394378Z","shell.execute_reply":"2023-11-27T02:35:58.010257Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorFlow in /opt/conda/lib/python3.10/site-packages (2.13.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.57.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (16.0.6)\nRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.24.3)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorFlow) (0.34.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorFlow) (0.41.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (3.4.4)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorFlow) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorFlow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorFlow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorFlow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorFlow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorFlow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorFlow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorFlow) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorFlow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorFlow) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorFlow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorFlow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorFlow) (3.2.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:48:38.891697Z","iopub.execute_input":"2023-11-27T02:48:38.892065Z","iopub.status.idle":"2023-11-27T02:48:38.896529Z","shell.execute_reply.started":"2023-11-27T02:48:38.892038Z","shell.execute_reply":"2023-11-27T02:48:38.895828Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"## Architecture of Neural Network\n# There are two types of model \nmodel = Sequential()\n\n# We will using 3 nodes, We use add function\n# Dense function is used to create layer\n\n# This is input layer\n# model.add(Dense(3, activation = 'relu', input_dim = 11)) #As we have 11 input columns\nmodel.add(Dense(11, activation = 'relu', input_dim = 11)) \n\n# hidden layer\nmodel.add(Dense(11, activation = 'relu', input_dim = 11)) #As we have 11 input columns\n\n# this is output layer\nmodel.add(Dense(1, activation = 'sigmoid'))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:48:38.897758Z","iopub.execute_input":"2023-11-27T02:48:38.898294Z","iopub.status.idle":"2023-11-27T02:48:38.950278Z","shell.execute_reply.started":"2023-11-27T02:48:38.898233Z","shell.execute_reply":"2023-11-27T02:48:38.949047Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:48:38.951986Z","iopub.execute_input":"2023-11-27T02:48:38.952260Z","iopub.status.idle":"2023-11-27T02:48:38.969901Z","shell.execute_reply.started":"2023-11-27T02:48:38.952235Z","shell.execute_reply":"2023-11-27T02:48:38.968876Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 11)                132       \n                                                                 \n dense_5 (Dense)             (None, 11)                132       \n                                                                 \n dense_6 (Dense)             (None, 1)                 12        \n                                                                 \n=================================================================\nTotal params: 276 (1.08 KB)\nTrainable params: 276 (1.08 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model compilation stage  for gradient descent\n# As its a binary classification its binary cross entropy is used\n# model.compile(loss = 'binary_crossentropy', optimizer = 'Adam')\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:54:16.697616Z","iopub.execute_input":"2023-11-27T02:54:16.697929Z","iopub.status.idle":"2023-11-27T02:54:16.710160Z","shell.execute_reply.started":"2023-11-27T02:54:16.697908Z","shell.execute_reply":"2023-11-27T02:54:16.708834Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Training the data set\n# Epochs tells you the iteration for finding the weight\nmodel.fit(X_trained_scaled, y_train, epochs = 100, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:56:05.592487Z","iopub.execute_input":"2023-11-27T02:56:05.592897Z","iopub.status.idle":"2023-11-27T02:57:27.565241Z","shell.execute_reply.started":"2023-11-27T02:56:05.592864Z","shell.execute_reply":"2023-11-27T02:57:27.564430Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Epoch 1/100\n200/200 [==============================] - 1s 3ms/step - loss: 0.3121 - accuracy: 0.8739 - val_loss: 0.3086 - val_accuracy: 0.8687\nEpoch 2/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8742 - val_loss: 0.3108 - val_accuracy: 0.8662\nEpoch 3/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8734 - val_loss: 0.3123 - val_accuracy: 0.8656\nEpoch 4/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8755 - val_loss: 0.3123 - val_accuracy: 0.8694\nEpoch 5/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3106 - accuracy: 0.8744 - val_loss: 0.3158 - val_accuracy: 0.8600\nEpoch 6/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8745 - val_loss: 0.3165 - val_accuracy: 0.8581\nEpoch 7/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8731 - val_loss: 0.3161 - val_accuracy: 0.8656\nEpoch 8/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8767 - val_loss: 0.3165 - val_accuracy: 0.8625\nEpoch 9/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8752 - val_loss: 0.3185 - val_accuracy: 0.8600\nEpoch 10/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8753 - val_loss: 0.3195 - val_accuracy: 0.8606\nEpoch 11/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8752 - val_loss: 0.3177 - val_accuracy: 0.8606\nEpoch 12/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8730 - val_loss: 0.3269 - val_accuracy: 0.8600\nEpoch 13/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8723 - val_loss: 0.3224 - val_accuracy: 0.8625\nEpoch 14/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8750 - val_loss: 0.3214 - val_accuracy: 0.8600\nEpoch 15/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8772 - val_loss: 0.3192 - val_accuracy: 0.8612\nEpoch 16/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8722 - val_loss: 0.3256 - val_accuracy: 0.8600\nEpoch 17/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8756 - val_loss: 0.3202 - val_accuracy: 0.8600\nEpoch 18/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8753 - val_loss: 0.3201 - val_accuracy: 0.8612\nEpoch 19/100\n200/200 [==============================] - 1s 3ms/step - loss: 0.3091 - accuracy: 0.8758 - val_loss: 0.3201 - val_accuracy: 0.8662\nEpoch 20/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8770 - val_loss: 0.3204 - val_accuracy: 0.8644\nEpoch 21/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8747 - val_loss: 0.3211 - val_accuracy: 0.8625\nEpoch 22/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8748 - val_loss: 0.3226 - val_accuracy: 0.8606\nEpoch 23/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8752 - val_loss: 0.3198 - val_accuracy: 0.8656\nEpoch 24/100\n200/200 [==============================] - 1s 3ms/step - loss: 0.3088 - accuracy: 0.8741 - val_loss: 0.3219 - val_accuracy: 0.8581\nEpoch 25/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8745 - val_loss: 0.3248 - val_accuracy: 0.8569\nEpoch 26/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8748 - val_loss: 0.3214 - val_accuracy: 0.8644\nEpoch 27/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8755 - val_loss: 0.3214 - val_accuracy: 0.8625\nEpoch 28/100\n200/200 [==============================] - 1s 3ms/step - loss: 0.3086 - accuracy: 0.8741 - val_loss: 0.3234 - val_accuracy: 0.8575\nEpoch 29/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8725 - val_loss: 0.3226 - val_accuracy: 0.8612\nEpoch 30/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8742 - val_loss: 0.3295 - val_accuracy: 0.8562\nEpoch 31/100\n200/200 [==============================] - 1s 3ms/step - loss: 0.3087 - accuracy: 0.8756 - val_loss: 0.3246 - val_accuracy: 0.8575\nEpoch 32/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8748 - val_loss: 0.3223 - val_accuracy: 0.8637\nEpoch 33/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8763 - val_loss: 0.3232 - val_accuracy: 0.8612\nEpoch 34/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8786 - val_loss: 0.3237 - val_accuracy: 0.8612\nEpoch 35/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8758 - val_loss: 0.3218 - val_accuracy: 0.8631\nEpoch 36/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8764 - val_loss: 0.3254 - val_accuracy: 0.8587\nEpoch 37/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8748 - val_loss: 0.3241 - val_accuracy: 0.8587\nEpoch 38/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8752 - val_loss: 0.3275 - val_accuracy: 0.8587\nEpoch 39/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8753 - val_loss: 0.3268 - val_accuracy: 0.8594\nEpoch 40/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8755 - val_loss: 0.3251 - val_accuracy: 0.8625\nEpoch 41/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8753 - val_loss: 0.3248 - val_accuracy: 0.8594\nEpoch 42/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8764 - val_loss: 0.3267 - val_accuracy: 0.8619\nEpoch 43/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8773 - val_loss: 0.3275 - val_accuracy: 0.8556\nEpoch 44/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8744 - val_loss: 0.3251 - val_accuracy: 0.8594\nEpoch 45/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8741 - val_loss: 0.3244 - val_accuracy: 0.8600\nEpoch 46/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8745 - val_loss: 0.3271 - val_accuracy: 0.8587\nEpoch 47/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8745 - val_loss: 0.3241 - val_accuracy: 0.8581\nEpoch 48/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8727 - val_loss: 0.3247 - val_accuracy: 0.8625\nEpoch 49/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8739 - val_loss: 0.3270 - val_accuracy: 0.8581\nEpoch 50/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8747 - val_loss: 0.3277 - val_accuracy: 0.8631\nEpoch 51/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8770 - val_loss: 0.3279 - val_accuracy: 0.8612\nEpoch 52/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8756 - val_loss: 0.3276 - val_accuracy: 0.8612\nEpoch 53/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8775 - val_loss: 0.3272 - val_accuracy: 0.8594\nEpoch 54/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8753 - val_loss: 0.3310 - val_accuracy: 0.8594\nEpoch 55/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8763 - val_loss: 0.3282 - val_accuracy: 0.8575\nEpoch 56/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8759 - val_loss: 0.3277 - val_accuracy: 0.8581\nEpoch 57/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8742 - val_loss: 0.3279 - val_accuracy: 0.8575\nEpoch 58/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8739 - val_loss: 0.3271 - val_accuracy: 0.8581\nEpoch 59/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8755 - val_loss: 0.3311 - val_accuracy: 0.8562\nEpoch 60/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8753 - val_loss: 0.3267 - val_accuracy: 0.8587\nEpoch 61/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8764 - val_loss: 0.3268 - val_accuracy: 0.8550\nEpoch 62/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8759 - val_loss: 0.3294 - val_accuracy: 0.8556\nEpoch 63/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8781 - val_loss: 0.3289 - val_accuracy: 0.8594\nEpoch 64/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8742 - val_loss: 0.3276 - val_accuracy: 0.8587\nEpoch 65/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8755 - val_loss: 0.3316 - val_accuracy: 0.8556\nEpoch 66/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8745 - val_loss: 0.3302 - val_accuracy: 0.8562\nEpoch 67/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8761 - val_loss: 0.3274 - val_accuracy: 0.8575\nEpoch 68/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8770 - val_loss: 0.3268 - val_accuracy: 0.8587\nEpoch 69/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8744 - val_loss: 0.3284 - val_accuracy: 0.8606\nEpoch 70/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8750 - val_loss: 0.3274 - val_accuracy: 0.8575\nEpoch 71/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8794 - val_loss: 0.3275 - val_accuracy: 0.8600\nEpoch 72/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8750 - val_loss: 0.3305 - val_accuracy: 0.8556\nEpoch 73/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8753 - val_loss: 0.3283 - val_accuracy: 0.8600\nEpoch 74/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8752 - val_loss: 0.3304 - val_accuracy: 0.8550\nEpoch 75/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8761 - val_loss: 0.3309 - val_accuracy: 0.8550\nEpoch 76/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8747 - val_loss: 0.3303 - val_accuracy: 0.8556\nEpoch 77/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8752 - val_loss: 0.3276 - val_accuracy: 0.8625\nEpoch 78/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8744 - val_loss: 0.3289 - val_accuracy: 0.8562\nEpoch 79/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8748 - val_loss: 0.3301 - val_accuracy: 0.8594\nEpoch 80/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8781 - val_loss: 0.3285 - val_accuracy: 0.8562\nEpoch 81/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8747 - val_loss: 0.3279 - val_accuracy: 0.8556\nEpoch 82/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8752 - val_loss: 0.3309 - val_accuracy: 0.8550\nEpoch 83/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8763 - val_loss: 0.3280 - val_accuracy: 0.8569\nEpoch 84/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8752 - val_loss: 0.3292 - val_accuracy: 0.8562\nEpoch 85/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8770 - val_loss: 0.3298 - val_accuracy: 0.8581\nEpoch 86/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8766 - val_loss: 0.3284 - val_accuracy: 0.8600\nEpoch 87/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8759 - val_loss: 0.3283 - val_accuracy: 0.8575\nEpoch 88/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8753 - val_loss: 0.3326 - val_accuracy: 0.8525\nEpoch 89/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8775 - val_loss: 0.3317 - val_accuracy: 0.8606\nEpoch 90/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8773 - val_loss: 0.3314 - val_accuracy: 0.8550\nEpoch 91/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8783 - val_loss: 0.3300 - val_accuracy: 0.8575\nEpoch 92/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8759 - val_loss: 0.3306 - val_accuracy: 0.8550\nEpoch 93/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8761 - val_loss: 0.3300 - val_accuracy: 0.8581\nEpoch 94/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8756 - val_loss: 0.3314 - val_accuracy: 0.8550\nEpoch 95/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8748 - val_loss: 0.3285 - val_accuracy: 0.8606\nEpoch 96/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8753 - val_loss: 0.3297 - val_accuracy: 0.8569\nEpoch 97/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8755 - val_loss: 0.3285 - val_accuracy: 0.8556\nEpoch 98/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8773 - val_loss: 0.3287 - val_accuracy: 0.8594\nEpoch 99/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8745 - val_loss: 0.3297 - val_accuracy: 0.8550\nEpoch 100/100\n200/200 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8767 - val_loss: 0.3289 - val_accuracy: 0.8587\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7c9f54a94790>"},"metadata":{}}]},{"cell_type":"code","source":"# Weights of input layer\n\nmodel.layers[0].get_weights()\n# We can see in the output that we have 33 layers\n# the last three are the bias\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:05:36.647004Z","iopub.execute_input":"2023-11-27T03:05:36.647320Z","iopub.status.idle":"2023-11-27T03:05:36.656362Z","shell.execute_reply.started":"2023-11-27T03:05:36.647292Z","shell.execute_reply":"2023-11-27T03:05:36.655270Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"[array([[-0.09976168,  0.5907575 , -0.06169327,  0.06382576, -0.06673948,\n         -0.3879387 , -0.13068189, -0.45897993, -0.3729103 , -0.06840406,\n         -0.02330129],\n        [-0.5752374 , -0.23860663,  1.412804  , -0.3949412 ,  0.17530364,\n         -0.9342807 , -0.04648294, -1.1501329 ,  0.31517476,  0.01891734,\n          0.3721983 ],\n        [-0.24365771,  0.5667581 , -0.39467323, -0.30561328, -0.35760954,\n         -0.24938153,  0.24110314, -0.18089035,  0.22273153,  0.014609  ,\n         -0.19179362],\n        [-0.6168819 , -0.5664822 , -0.28956312, -0.664835  ,  0.10138931,\n         -0.20890114, -0.8558839 ,  0.30076814, -0.17984882,  1.4286057 ,\n          0.6788362 ],\n        [ 0.04308292, -0.509516  ,  0.26605505, -0.08209921, -1.1639907 ,\n         -0.0495824 , -0.28237036,  0.3957846 , -1.3569072 , -0.05549704,\n          1.0990909 ],\n        [-0.04447197,  0.0069619 , -0.1985656 , -0.10836428, -0.26267448,\n         -0.24999098,  0.03559808,  0.3329009 ,  0.1139055 , -0.02016657,\n         -0.08133315],\n        [-0.45559683,  0.40876603, -0.02709536, -0.41731122, -0.47205505,\n         -0.4214353 ,  0.20360044,  0.32089546, -0.36441323,  0.03774626,\n         -0.39811215],\n        [ 0.4828152 ,  0.4629102 ,  0.13928382,  0.31168523, -0.12154139,\n         -0.60533434, -0.20629734,  0.18109247,  0.10523442,  0.11801435,\n          0.08950177],\n        [ 0.09557581, -0.1946691 ,  0.3139226 ,  0.4481234 , -0.0841314 ,\n          0.32077932,  0.04095332, -0.25524023,  0.30807778, -0.86049134,\n         -0.47489116],\n        [ 0.6390923 ,  0.07659104, -0.10206286, -1.1109954 , -0.12577511,\n          0.22160657,  0.3960493 , -0.3280802 , -0.04208729, -0.1603773 ,\n         -0.02051599],\n        [ 0.09280456, -0.24933253, -0.12500551,  0.02572848,  0.19757594,\n          0.09631994,  0.37335783,  0.47863117, -0.12629391,  0.0286486 ,\n         -0.1063353 ]], dtype=float32),\n array([ 0.9757007 ,  0.4160858 , -0.35194558,  0.965837  ,  0.12820517,\n         0.8896092 ,  0.92092127,  0.7960077 ,  0.21606684,  0.41470695,\n        -0.04157804], dtype=float32)]"},"metadata":{}}]},{"cell_type":"code","source":"# Weights of output layer layer\n\nmodel.layers[1].get_weights()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:05:40.159887Z","iopub.execute_input":"2023-11-27T03:05:40.160270Z","iopub.status.idle":"2023-11-27T03:05:40.168508Z","shell.execute_reply.started":"2023-11-27T03:05:40.160245Z","shell.execute_reply":"2023-11-27T03:05:40.167257Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"[array([[-0.37427804, -0.94035614,  0.4440322 , -0.9671852 ,  0.3940252 ,\n         -0.6069511 , -0.34447482, -0.2789808 ,  0.21505204,  0.41243318,\n         -0.3726318 ],\n        [ 0.4117142 ,  0.098213  ,  0.5345253 ,  0.06085334,  0.36949402,\n         -0.07966939, -0.17961596,  0.7018631 ,  0.15575069, -0.15555523,\n          0.2103123 ],\n        [ 0.50320286,  0.37911424,  0.20196399,  0.39372906,  0.25796458,\n          0.8891484 , -0.7185121 ,  0.29381528,  0.45240825, -1.0398448 ,\n          0.2021232 ],\n        [ 0.28019324,  0.51424617, -0.827046  ,  0.66750914, -0.19331191,\n         -1.6213198 ,  0.03181925, -0.11124533, -0.56921273,  0.6461105 ,\n         -0.5857105 ],\n        [-0.79017246,  0.20445655, -0.75796616, -0.60230684,  0.539425  ,\n          0.32310122, -0.647655  ,  0.2988864 ,  0.13595472,  0.03824025,\n          0.6522069 ],\n        [-0.09066954, -0.1002325 , -0.90428144, -0.21294142, -0.27558684,\n         -1.1511121 , -0.5098058 , -0.30187625, -1.233524  , -0.01168693,\n         -0.47566304],\n        [ 0.5223253 ,  0.71663374,  0.21301997,  0.21392056,  0.00188195,\n          0.19860666,  0.4031295 , -0.18232548, -0.1569632 ,  0.5211368 ,\n          0.6141784 ],\n        [ 0.49633908,  0.31025964,  0.14098516,  0.37335634, -0.07795396,\n          0.8201755 ,  0.580748  ,  0.5319073 ,  0.5365134 , -0.39583847,\n          0.36462474],\n        [ 0.47405472, -0.68306637,  0.60959756,  0.3850303 , -1.7260782 ,\n          0.4469378 , -0.23920497,  0.6705831 ,  0.61543477,  0.1055498 ,\n          0.46681204],\n        [ 0.33992562,  0.16727531, -0.5953636 , -0.02376092, -0.00234803,\n         -0.35246384, -0.12814072, -1.4020072 ,  0.46849775,  0.258784  ,\n         -0.89140105],\n        [-0.36020303, -0.04006281, -0.00385431,  0.10053145,  0.13598527,\n         -0.2609777 ,  0.28837174,  0.7100689 ,  0.9695571 , -0.11795157,\n          1.0199226 ]], dtype=float32),\n array([ 0.12663852,  0.36670545,  0.1075265 , -0.15298013,  0.24137636,\n        -0.5463091 ,  0.34759924,  0.62785524, -0.4344626 ,  0.49609384,\n         0.04453582], dtype=float32)]"},"metadata":{}}]},{"cell_type":"code","source":"y_log = model.predict(X_test_scaled)\n# as we are using sigmoid function, the resulting value will be probability ]\n# We need to convert probability into binary values, to achieve that we need to have\n# threshold\n# in this example will be taking 0.5 as the threshold, so anything below 0.5 will be consdiered as 0 and above 0.5 will be 1\n# we need to use AUC and Roc curve to find the optimum threshold\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:05:42.690431Z","iopub.execute_input":"2023-11-27T03:05:42.690808Z","iopub.status.idle":"2023-11-27T03:05:42.860148Z","shell.execute_reply.started":"2023-11-27T03:05:42.690779Z","shell.execute_reply":"2023-11-27T03:05:42.859019Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"63/63 [==============================] - 0s 998us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = np.where(y_log > 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:05:45.403179Z","iopub.execute_input":"2023-11-27T03:05:45.404215Z","iopub.status.idle":"2023-11-27T03:05:45.409777Z","shell.execute_reply.started":"2023-11-27T03:05:45.404157Z","shell.execute_reply":"2023-11-27T03:05:45.408637Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T03:05:47.206445Z","iopub.execute_input":"2023-11-27T03:05:47.206749Z","iopub.status.idle":"2023-11-27T03:05:47.215275Z","shell.execute_reply.started":"2023-11-27T03:05:47.206726Z","shell.execute_reply":"2023-11-27T03:05:47.214021Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"0.8665"},"metadata":{}}]},{"cell_type":"markdown","source":"Changes we can do to increase the performance\n* Increase epochs.\n* We can increase the number of nodes in input layer.\n* We can use relu instead of sigmoid, as in general relu give better performance. Put it in input layer.\n* We can increase the number of layers.\n* Use validataion split to check the performance of the model\n\nValidation split\n* observation 1: We need to make sure in every epoch the loss is dropping and accuracy is increasing.\n* Observation 2: There is very important thing, we need to observe the validation accuracy and total accuracy together as if only total accuracy is increasing but validation accuracy is not then thats the case of overfitting.\n","metadata":{}}]}